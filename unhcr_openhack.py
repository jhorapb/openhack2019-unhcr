# -*- coding: utf-8 -*-
"""unhcr-openhack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1klHifnrODex5wLKaE87hUwC3kbbZ5WTY
"""

!pip install -q botometer

from openpyxl import load_workbook, Workbook
from botometer import Botometer
import pandas as pd
import random

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

def print_dataset(dataset):
  for row in twitter_dataset.itertuples():
    print(row)
    # print(row[1])

def sort_dataset(dataset, col_names, ascending_vals):
  # sorting data frame by name
  # col_names.append('rank')
  # ascending_vals.append(True)
  dataset.sort_values(col_names, axis = 0, ascending = ascending_vals, 
                      inplace = True, na_position ='last')
  
def filter_dataset(dataset, col_name, col_value):
  # filtering data frame by name
  col_name = col_name.lower()
  if col_name in dataset.columns:
    return dataset[dataset[col_name].str.lower().isin([col_value.casefold()])]
  return dataset

def set_users_bot_prob(users_list):
  global users_bot_prob
  for user in users_list:
    if user not in users_bot_prob:
      bot_prob = random.uniform(0, 1)
      if bot_prob >= 0.6:
        users_bot_prob[user] = bot_prob

def discard_tweet_bots(dataset):
  global users_bot_prob
  set_users_bot_prob(dataset['author id'].unique())
  print(users_bot_prob)
  return dataset[~dataset['author id'].isin(users_bot_prob)]

def add_keyword_ranking(dataset):
  global keywords_data
  print(keywords_data)
  for keyword in keywords_data.itertuples():
    mask = (dataset['themes'].str.lower().str.contains(keyword[1].lower(), na=False))
    dataset['rank'][mask] = keyword[0]

users_bot_prob = {}
config_file2 = pd.read_csv('config.csv', header=None)
keywords_data = pd.read_csv('keywords.csv', header=None)
data_file = pd.ExcelFile('sol_sample_uhrcn.xlsx')

# Creating dataframe from Twitter data sheet
twitter_dataset = data_file.parse(data_file.sheet_names[0])
twitter_dataset.columns = twitter_dataset.columns.str.lower()
twitter_dataset['rank'] = ''
print('Initial shape:', twitter_dataset.shape)
# print(twitter_dataset.columns)

add_keyword_ranking(twitter_dataset)

twitter_dataset = discard_tweet_bots(twitter_dataset)
print('NO BOTS shape:', twitter_dataset.shape)
print('TOTAL BOTS:', len(users_bot_prob))
# Read the sorting commands from config file
list_sort_cols = []
list_sort_as_des = []
for command in config_file2.itertuples():
  if command[1].lower() == 'sortby':
    col_name = command[2].lower()
    if col_name in twitter_dataset.columns:
      list_sort_cols.append(col_name)
      list_sort_as_des.append(command[3].lower() == 'a')

if list_sort_cols:
  sort_dataset(twitter_dataset, list_sort_cols, list_sort_as_des)

# Process commands from config file
for command in config_file2.itertuples():
  if command[1].lower() != 'sortby':
    if command[1].lower() == 'filterby':
      twitter_dataset = filter_dataset(twitter_dataset, command[2], command[3])

# print(twitter_dataset.shape)

# Output file
twitter_dataset.to_excel("optimised_file.xlsx")

twitter_dataset